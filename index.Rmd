---
output: 
  html_document: 
    keep_md: yes
---
output: 
  html_document: 
    keep_md: yes
  
title: "Practical Machine Learning: Course Project "
author: "Barbara L. Clayton"
date: "March 19, 2016"


###Machine Learning: Course Project on Human Activity Recognition

##Overview
Human Activity Recognition (HAR) is a key research area that is gaining increasing attention, especially for the development of context-aware systems. Devices used to capture HAR are becoming more and more common, and are able to collect a large amount of data about people and their personal health activities. The goal of this project is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did the exercise. To do this we will use machine learning to build an accurate model to predict the manner of exercise.

This report provides the considerations taken into account in selecting which models to test, how the models were built, cross validation used, and the expected out of sample error for models which was used to select the model for predicting Class for the testing data.  

#####Data Source

The data for this project comes from 
[Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201) 

*Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. “Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human ’13)”. Stuttgart, Germany: ACM SIGCHI, 2013.*

My thanks to the authors for allowing their data to be used for this analysis. 

From the description the authors provided in their paper, this sample data was captured from a device used by test subjects doing weight lifting activities.  The data was grouped into five discrete classes in the ‘classe’ variable (Class A, B, C, D, E). Class A denotes the execution of the activity as specified, while the other four classes correspond to common mistakes. 

The data will be used in this paper to build a model to predict the class of movement.  The model will be used to predict the class of movement for 20 test cases.

####Setting up working Environment
Use the R libraries that are necessary for this analysis and set working directory. See Appendix, *Set Up* to review code

```{r setup, ref.label="setup1", echo=FALSE, results='hide', message=FALSE }
```

###Preparing Data for Modeling 
Load the training and testing datasets from URLs provided. See Appendix, *Data Download* to review code

```{r down, ref.label="down1", echo=FALSE, results='hide', message=FALSE, warning=FALSE}
```



The training dataset is then partioned to create a data set used to train the model (70% of the data) and a data set to validate the model (remaining 30%). The testing dataset is not changed and will only be used for the quiz results generation.  

```{r partitionData}
# create a partition with the training dataset
set.seed(54321)
inTrain  <- createDataPartition(training$classe, p=0.7, list=FALSE)
TrainData <- training[inTrain, ]
TestData  <- training[-inTrain, ]
rows<-160
```

####Removing Variables from the Model
Both datasets start with `r rows` variables and a similar distribution across the classes. 

In reviewing the data we found varaibles that should be removed due to: 
* Near Zero Variance variables 
* Variables which have >95% NA values
* Seven ID variables that will not contribute to a useful model.

We remove these variables. See Appendix, *Cleaning Data* to review code and for the dimention and distribution of test data and train data before and after removing variables

```{r removeVar, ref.label="removeVar1", echo=FALSE,results='hide', message=FALSE, warning=FALSE}
```

After removing these variables we have 52 variables in the train and test data and the distribution across classes is not changed.
```{r dataClean}
dim(TrainData)
dim(TrainData)
summary(TrainData$classe)
summary(TestData$classe)
``` 

#### Analysis of Data  
* The variable we want to predict is a qualitative variable in that it is a class of data.  The predictor variables are quantitative. 

* The data is not high dimentional data as the number of observations is > the number of variables.

* An analysis of correlation shows few correlations between predictor variables

```{r Corr, message=FALSE, warning=FALSE}
corDat <- cor(TrainData[, -52])
corrplot(corDat, order = "FPC", method = "ellipse", type = "lower", 
         tl.cex = 0.6, tl.col = rgb(0, 0, 0))
```


####Prediction Models
Considerations on Model selection:

* Given this is a classification setting, consider models that approximate the Bayes classifer and estimate the conditional distribution of X (variable to predict) given Y (the predictor varaibles) 
* The model choice must take into account the balance of flexibility versus bias. More flexible methods can lead to over-fit of the model
* Model interpretation importance
* Prediction accuracy.  Due to need for acccuracy, random forest and boosting were selected to trade off of some interpretability for accuracy


Use trainControl to set the cross validation method and number of resampling iterations
```{r trainset, message=FALSE, warning=FALSE}
tc <- trainControl(method = "cv", number = 5, verboseIter=FALSE , allowParallel=TRUE)
tcgbm <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
set.seed(54321) 
```

Three models selected for comparison: Random Forest, Bayes Generalized Linear Model, Generalized Boosted Model

```{r models, results='hide', message=FALSE, warning=FALSE}

rfModel <- train(classe ~ ., data=TrainData, method="rf",trControl=tc)
bgModel <- train(classe ~ ., data = TrainData, method = "bayesglm", trControl=tc)
gbmModel  <- train(classe ~ ., data=TrainData, method = "gbm",trControl = tcgbm)

```

To determine the best model accuracy and Kappa are compared.  Kappa (interrelator relibility) represents the extent to which the data collected in the study are correct representations of the variables measured. 
```{r modelcompare,results='hide', message=FALSE, warning=FALSE}
model <- c("Random Forest","BayesGLM","Generalized Boosted Model")
Accuracy <- c(max(rfModel$results$Accuracy),
        max(bgModel$results$Accuracy),
        max(gbmModel$results$Accuracy))
        
Kappa <- c(max(rfModel$results$Kappa),
        max(bgModel$results$Kappa),
        max(gbmModel$results$Kappa))

performance <- cbind(model,Accuracy,Kappa)

```

```{r table}
knitr::kable(performance)
```

The accuracy of the Random Forests model is  
**`r max(rfModel$results$Accuracy)`**. 

The out of sample error is calculated as 1 - accuracy for predictions made against the cross-validation set. Given the accuracy rate is above 99% few if any of the test samples will be mis-classified.
###Apply the Prediction Model to Testing Data
Given the Random Forest model was the most accurate it will be applied to predict the 20 quiz results as shown below.From the model

```{r PredictQuiz}
predictQuiz <- predict(rfModel, testing)
predictQuiz
```


\pagebreak

####Appendix

####Set Up
Calling libraries and reading in data and set working directory
```{r setup1}
library(arm) 
library(caret) 
library(class) 
library(rpart) 
library(rpart.plot) 
library(rattle) 
library(randomForest) 
library(corrplot) 
setwd("~/Desktop/Data Science Cert Homework/Machine Learning") 
```

####Data Download
```{r down1, results='hide', message=FALSE, warning=FALSE}
# define urls to use
urlTrain <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"  
urlTest  <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"  
# download the datasets
training <- read.csv(url(urlTrain))  
testing  <- read.csv(url(urlTest))  
``` 

#### Cleaning Data

We remove variables with nearly zero variance as detected with the NZV function, variables with >95% NA values and the first 7 columns which are ID variables.

Here is the data before removing variables
```{r DataBefore, message=FALSE, warning=FALSE}
dim(TrainData) 
dim(TestData)
summary(TrainData$classe)
summary(TestData$classe)  
```

Removing variables
```{r removeVar1, message=FALSE, warning=FALSE}
# remove variables with Nearly Zero Variance
NearZ <- nearZeroVar(TrainData) 
TrainData <- TrainData[, -NearZ] 
TestData  <- TestData[, -NearZ] 
# remove variables that are >95% NA
NADat    <- sapply(TrainData, function(x) mean(is.na(x))) > 0.95 
TrainData <- TrainData[, NADat==FALSE] 
TestData  <- TestData[, NADat==FALSE] 
# remove identification only variables (columns 1 to 7)
TrainData <- TrainData[, -(1:7)] 
TestData  <- TestData[, -(1:7)]
```

 
Here is the data after removing variables
```{r DataAfter, message=FALSE, warning=FALSE}
dim(TrainData) 
dim(TestData)
summary(TrainData$classe)
summary(TestData$classe) 
```


